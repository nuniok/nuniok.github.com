<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)">
<meta name="generator" content="Hexo 7.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/resource/img/fav2024.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/resource/img/fav2024.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/resource/img/fav2024.png">
  <link rel="mask-icon" href="/resource/img/fav2024.png" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"noge.top","root":"/","images":"/images","scheme":"Pisces","darkmode":true,"version":"8.8.2","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2929408904388376" crossorigin="anonymous"></script><meta property="og:type" content="website">
<meta property="og:title" content="码力欧">
<meta property="og:url" content="https://noge.top/page/8/index.html">
<meta property="og:site_name" content="码力欧">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="nuniok">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://noge.top/page/8/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/8/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>码力欧</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?9cdc9a11cbd242c6336b07c464d8820c"></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">码力欧</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">的业余生活</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a></li>
        <li class="menu-item menu-item-career"><a href="/career/" rel="section"><i class="sitemap fa-fw"></i>├ 技术</a></li>
        <li class="menu-item menu-item-life"><a href="/life/" rel="section"><i class="sitemap fa-fw"></i>├ 生活</a></li>
        <li class="menu-item menu-item-future"><a href="/future/" rel="section"><i class="sitemap fa-fw"></i>└ 未来</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">nuniok</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">122</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">93</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/nuniok" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;nuniok" rel="noopener" target="_blank"><i class="github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:nuniok@163.com" title="E-Mail → mailto:nuniok@163.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i></a>
      </span>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/11/18/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/11/18/1.html" class="post-title-link" itemprop="url">深度学习系列第五篇 — 卷积神经网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-11-18 00:00:00" itemprop="dateCreated datePublished" datetime="2017-11-18T00:00:00+00:00">2017-11-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">卷积神经网络 - 维基百科</a></p>
<p>这篇例子是学习 <a target="_blank" rel="noopener" href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/">莫烦PYTHON</a> 视频教程整理的学习笔记。</p>
<p>卷积神经网络包含如下几层：</p>
<ol>
<li>输入层</li>
<li>卷积层（卷积层的结构 + 向前传播算法） &#x3D;&gt;&gt; （过滤器或内核）（用来提取特征,每一层会将图层变厚）</li>
<li>池化层（用来采样，稀疏参数，每一层会将图层变瘦）</li>
<li>全连接层（将前两层提取的图像特征使用全连接层完成分类任务）</li>
<li>Softmax 层（主要处理分类，得到不同种类的概率分布情况）</li>
</ol>
<p><img src="/resource/img/cnn.jpg" alt="cnn"></p>
<p>首先我们将如下包导入，这次练习是通过 Tensorflow 提供的 MNIST 数据集进行训练，识别手写数字。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载或加载数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&quot;MNIST_data&quot;</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">weight_variable</span>(<span class="params">shape</span>):</span><br><span class="line">    inital = tf.truncated_normal(shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(inital)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bias_variable</span>(<span class="params">shape</span>):</span><br><span class="line">    inital = tf.constant(<span class="number">0.1</span>, shape=shape)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(inital)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_layer</span>(<span class="params">inputs, in_size, out_size, activation_function=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 在生成初始参数时，随机变量(normal distribution)会比全部为0要好很多</span></span><br><span class="line">    <span class="comment"># 所以我们这里的weights为一个in_size行, out_size列的随机变量矩阵。</span></span><br><span class="line">    weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    <span class="comment"># biases的推荐值不为0，所以我们这里是在0向量的基础上又加了0.1</span></span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line">    wx_plus_b = tf.matmul(inputs, weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        outputs = wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_accuracy</span>(<span class="params">v_xs, v_ys</span>):</span><br><span class="line">    <span class="keyword">global</span> prediction</span><br><span class="line">    y_pre = sess.run(prediction, feed_dict=&#123;xs: v_xs, keep_drop: <span class="number">1</span>&#125;)</span><br><span class="line">    correct_prediction = tf.equal(tf.argmax(y_pre,<span class="number">1</span>), tf.argmax(v_ys,<span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs: v_xs, ys: v_ys, keep_drop: <span class="number">1</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv2d</span>(<span class="params">x, w</span>):</span><br><span class="line">    <span class="comment"># stride [1, x_movement, y_movement, 1]</span></span><br><span class="line">    <span class="comment"># Must have strides[0] = strides[4] = 1</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, w, strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">max_pool_2x2</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># stride [1, x_movement, y_movement, 1]</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line"></span><br><span class="line">xs = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])  <span class="comment"># 28 * 28</span></span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">keep_drop = tf.placeholder(tf.float32)</span><br><span class="line">x_image = tf.reshape(xs, [-<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"><span class="comment"># print x_image.shape  # [n_sample, 28, 28, 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># conv1 layer #</span></span><br><span class="line">w_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])  <span class="comment"># patch 5 x 5, in size 1, out size 32</span></span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)  <span class="comment"># output size 28 x 28 x 32</span></span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)  <span class="comment"># output size 14 x 14 x 32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># conv2 layer #</span></span><br><span class="line">w_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])  <span class="comment"># patch 5 x 5, in size 32, out size 64</span></span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)  <span class="comment"># output size 14 x 14 x 64</span></span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)  <span class="comment"># output size 7 x 7 x 64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># func1 layer #</span></span><br><span class="line">w_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [-<span class="number">1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])  <span class="comment"># [n_sample, 7, 7 64]  -&gt; [n_sample, 7*7*64]</span></span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_drop)</span><br><span class="line"></span><br><span class="line"><span class="comment"># func2 layer #</span></span><br><span class="line">w_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<p>通过使用 <code>softmax</code> 分类器输出分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)</span><br></pre></td></tr></table></figure>

<p>loss函数（即最优化目标函数）选用交叉熵函数。交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，它们的交叉熵等于零。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>

<p>通过使用 <code>AdamOptimizer</code> 优化器进行优化，使 <code>cross_entropy</code> 最小</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy)</span><br></pre></td></tr></table></figure>

<p>下面开始训练，通过两千次的训练，每次抽样 100 条数据，每 100 次训练完验证一下预测的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">2001</span>):</span><br><span class="line">        batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys, keep_drop: <span class="number">0.5</span>&#125;)</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span> i, compute_accuracy(</span><br><span class="line">                mnist.test.images[:<span class="number">1000</span>], mnist.test.labels[:<span class="number">1000</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> <span class="string">&quot;end!&quot;</span></span><br></pre></td></tr></table></figure>
<p>训练结果：</p>
<pre><code>100 0.861
200 0.904
300 0.93
400 0.933
500 0.94
600 0.949
700 0.955
800 0.958
900 0.961
1000 0.964
1100 0.966
1200 0.968
1300 0.972
1400 0.968
1500 0.973
1600 0.972
1700 0.976
1800 0.975
1900 0.973
2000 0.981
end!
</code></pre>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/11/17/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/11/17/1.html" class="post-title-link" itemprop="url">Tornado 异步非阻塞浅析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-11-17 00:00:00" itemprop="dateCreated datePublished" datetime="2017-11-17T00:00:00+00:00">2017-11-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>[以下代码基于 Tornado 3.2.1 版本讲解]<br>[主要目标：讲解 gen.coroutine、Future、Runner 之间的关系]</p>
<p>这里是示例运行代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python</span><br><span class="line"># coding: utf-8</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">File: demo.py</span><br><span class="line">Author: noogel</span><br><span class="line">Date: 2017-08-28 22:59</span><br><span class="line">Description: demo</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">import tornado</span><br><span class="line"></span><br><span class="line">from tornado import gen, web</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@gen.coroutine</span><br><span class="line">def service_method():</span><br><span class="line">    raise gen.Return(&quot;abc&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NoBlockHandler(tornado.web.RequestHandler):</span><br><span class="line"></span><br><span class="line">    @web.asynchronous</span><br><span class="line">    @gen.coroutine</span><br><span class="line">    def get(self):</span><br><span class="line">        result = yield service_method()</span><br><span class="line">        self.write(result)</span><br><span class="line">        self.finish()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Application(tornado.web.Application):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        settings = &#123;</span><br><span class="line">            &quot;xsrf_cookies&quot;: False,</span><br><span class="line">        &#125;</span><br><span class="line">        handlers = [</span><br><span class="line">            (r&quot;/api/noblock&quot;, NoBlockHandler),</span><br><span class="line">        ]</span><br><span class="line">        tornado.web.Application.__init__(self, handlers, **settings)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    Application().listen(2345)</span><br><span class="line">    tornado.ioloop.IOLoop.instance().start()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>演示运行效果…</p>
</blockquote>
<p>讲解从 coroutine 修饰器入手，这个函数实现了简单的异步，它通过 generator 中的 yield 语句使函数暂停执行，将中间结果临时保存，然后再通过 send() 函数将上一次的结果送入函数恢复函数执行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def coroutine(func):</span><br><span class="line">    @functools.wraps(func)</span><br><span class="line">    def wrapper(*args, **kwargs):</span><br><span class="line">        future = TracebackFuture()</span><br><span class="line">        if &#x27;callback&#x27; in kwargs:</span><br><span class="line">            print(&quot;gen.coroutine callback:&#123;&#125;&quot;.format(kwargs[&#x27;callback&#x27;]))</span><br><span class="line">            callback = kwargs.pop(&#x27;callback&#x27;)</span><br><span class="line">            IOLoop.current().add_future(</span><br><span class="line">                future, lambda future: callback(future.result()))</span><br><span class="line">        try:</span><br><span class="line">            print(&quot;gen.coroutine run func:&#123;&#125;&quot;.format(func))</span><br><span class="line">            result = func(*args, **kwargs)</span><br><span class="line">        except (Return, StopIteration) as e:</span><br><span class="line">            result = getattr(e, &#x27;value&#x27;, None)</span><br><span class="line">        except Exception:</span><br><span class="line">            future.set_exc_info(sys.exc_info())</span><br><span class="line">            return future</span><br><span class="line">        else:</span><br><span class="line">            if isinstance(result, types.GeneratorType):</span><br><span class="line">                def final_callback(value):</span><br><span class="line">                    deactivate()</span><br><span class="line">                    print(&quot;gen.coroutine final set_result:&#123;&#125;&quot;.format(value))</span><br><span class="line">                    future.set_result(value)</span><br><span class="line">                print(&quot;gen.coroutine will Runner.run() result:&#123;&#125;&quot;.format(result))</span><br><span class="line">                runner = Runner(result, final_callback)</span><br><span class="line">                runner.run()</span><br><span class="line">                return future</span><br><span class="line">        print(&quot;@@ gen.coroutine will set_result and return:&#123;&#125;&quot;.format(result))</span><br><span class="line">        future.set_result(result)</span><br><span class="line">        return future</span><br><span class="line">    return wrapper</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">st=&gt;start: create future object</span><br><span class="line">rf=&gt;operation: run function</span><br><span class="line">ex=&gt;condition: is not exception</span><br><span class="line">gen=&gt;condition: is generator</span><br><span class="line">run=&gt;operation: Runner.run()</span><br><span class="line">fts=&gt;operation: future.set_done()</span><br><span class="line">rtnf=&gt;operation: return future</span><br><span class="line">ed=&gt;end</span><br><span class="line"></span><br><span class="line">st-&gt;rf-&gt;ex</span><br><span class="line">ex(no)-&gt;rtnf</span><br><span class="line">ex(yes)-&gt;gen</span><br><span class="line">gen(yes)-&gt;run</span><br><span class="line">gen(no)-&gt;rtnf</span><br><span class="line">run-&gt;rtnf</span><br><span class="line">rtnf-&gt;ed</span><br></pre></td></tr></table></figure>


<p>首先创建一个Future实例，然后执行被修饰的函数，一般函数返回的是一个生成器对象，接下来交由 Runner 处理，如果函数返回的是 Return, StopIteration 那么表示函数执行完成将结果放入 future 中并 set_done() 返回。</p>
<p>下面是Future的简版：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class Future(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self._result = None</span><br><span class="line">        self._callbacks = []</span><br><span class="line"></span><br><span class="line">    def result(self, timeout=None):</span><br><span class="line">        self._clear_tb_log()</span><br><span class="line">        if self._result is not None:</span><br><span class="line">            return self._result</span><br><span class="line">        if self._exc_info is not None:</span><br><span class="line">            raise_exc_info(self._exc_info)</span><br><span class="line">        self._check_done()</span><br><span class="line">        return self._result</span><br><span class="line"></span><br><span class="line">    def add_done_callback(self, fn):</span><br><span class="line">        if self._done:</span><br><span class="line">            fn(self)</span><br><span class="line">        else:</span><br><span class="line">            self._callbacks.append(fn)</span><br><span class="line"></span><br><span class="line">    def set_result(self, result):</span><br><span class="line">        self._result = result</span><br><span class="line">        self._set_done()</span><br><span class="line"></span><br><span class="line">    def _set_done(self):</span><br><span class="line">        self._done = True</span><br><span class="line">        for cb in self._callbacks:</span><br><span class="line">            try:</span><br><span class="line">                cb(self)</span><br><span class="line">            except Exception:</span><br><span class="line">                app_log.exception(&#x27;Exception in callback %r for %r&#x27;, cb, self)</span><br><span class="line">        self._callbacks = None</span><br></pre></td></tr></table></figure>
<p>在tornado中大多数的异步操作返回一个Future对象，这里指的是 Runner 中处理的异步返回结果。我们可以将该对象抽象成一个占位对象，它包含很多属性和函数。一个 Future 对象一般对应这一个异步操作。当这个对象的异步操作完成后会通过 set_done() 函数去处理 _callbacks 中的回调函数，这个回调函数是在我们在做修饰定义的时候传入 coroutine 中的。</p>
<p>下面的代码是在 coroutine 中定义的，用来添加对异步操作完成后的回调处理。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">if &#x27;callback&#x27; in kwargs:</span><br><span class="line">    print(&quot;gen.coroutine callback:&#123;&#125;&quot;.format(kwargs[&#x27;callback&#x27;]))</span><br><span class="line">    callback = kwargs.pop(&#x27;callback&#x27;)</span><br><span class="line">    IOLoop.current().add_future(</span><br><span class="line">        future, lambda future: callback(future.result()))</span><br></pre></td></tr></table></figure>
<p>这里是 IOLoop 中的 add_future 函数，它是来给 future 对象添加回调函数的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def add_future(self, future, callback):</span><br><span class="line">    assert isinstance(future, Future)</span><br><span class="line">    callback = stack_context.wrap(callback)</span><br><span class="line">    future.add_done_callback(</span><br><span class="line">        lambda future: self.add_callback(callback, future))</span><br></pre></td></tr></table></figure>

<p>然后说 Runner 都做了什么。在 3.2.1 版本中 Runner 的作用更重要一些。那么 Runner() 的作用是什么？<br>它主要用来控制生成器的执行与终止，将异步操作的结果 send() 至生成器暂停的地方恢复执行。在生成器嵌套的时候，当 A 中 yield B 的时候，先终止 A 的执行去执行 B，然后当 B 执行结束后将结果 send 至 A 终止的地方继续执行 A。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">class Runner(object):</span><br><span class="line">    def __init__(self, gen, final_callback):</span><br><span class="line">        self.gen = gen</span><br><span class="line">        self.final_callback = final_callback</span><br><span class="line">        self.yield_point = _null_yield_point</span><br><span class="line">        self.results = &#123;&#125;</span><br><span class="line">        self.running = False</span><br><span class="line">        self.finished = False</span><br><span class="line"></span><br><span class="line">    def is_ready(self, key):</span><br><span class="line">        if key not in self.pending_callbacks:</span><br><span class="line">            raise UnknownKeyError(&quot;key %r is not pending&quot; % (key,))</span><br><span class="line">        return key in self.results</span><br><span class="line"></span><br><span class="line">    def set_result(self, key, result):</span><br><span class="line">        self.results[key] = result</span><br><span class="line">        self.run()</span><br><span class="line"></span><br><span class="line">    def pop_result(self, key):</span><br><span class="line">        self.pending_callbacks.remove(key)</span><br><span class="line">        return self.results.pop(key)</span><br><span class="line"></span><br><span class="line">    def run(self):</span><br><span class="line">        try:</span><br><span class="line">            self.running = True</span><br><span class="line">            while True:</span><br><span class="line">                next = self.yield_point.get_result()</span><br><span class="line">                self.yield_point = None</span><br><span class="line">                try:</span><br><span class="line">                    print(&quot;gen.Runner.run() will send(next)&quot;)</span><br><span class="line">                    yielded = self.gen.send(next)</span><br><span class="line">                    print(&quot;gen.Runner.run() send(next) done.&quot;)</span><br><span class="line">                except (StopIteration, Return) as e:</span><br><span class="line">                    print(&quot;gen.Runner.run() send(next) throw StopIteration or Return done.&quot;)</span><br><span class="line">                    self.finished = True</span><br><span class="line">                    self.yield_point = _null_yield_point</span><br><span class="line">                    self.final_callback(getattr(e, &#x27;value&#x27;, None))</span><br><span class="line">                    self.final_callback = None</span><br><span class="line">                    return</span><br><span class="line">                if isinstance(yielded, (list, dict)):</span><br><span class="line">                    yielded = Multi(yielded)</span><br><span class="line">                elif isinstance(yielded, Future):</span><br><span class="line">                    yielded = YieldFuture(yielded)</span><br><span class="line">                    self.yield_point = yielded</span><br><span class="line">                    self.yield_point.start(self)</span><br><span class="line">        finally:</span><br><span class="line">            self.running = False</span><br><span class="line"></span><br><span class="line">    def result_callback(self, key):</span><br><span class="line">        def inner(*args, **kwargs):</span><br><span class="line">            if kwargs or len(args) &gt; 1:</span><br><span class="line">                result = Arguments(args, kwargs)</span><br><span class="line">            elif args:</span><br><span class="line">                result = args[0]</span><br><span class="line">            else:</span><br><span class="line">                result = None</span><br><span class="line">            self.set_result(key, result)</span><br><span class="line">        return wrap(inner)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>实例化 Runner() 的时候将生成器对象和生成器执行结束时的回调函数传入，然后通过 run() 函数去继续执行生成器对象。</p>
<p>run() 函数的处理首先包了一层 while 循环，因为在生成器对象中可能包含多个 yield 语句。</p>
<p><code>yielded = self.gen.send(next)</code>，在第一次 send() 恢复执行的时候默认传入 None ,因为函数第一次执行并没有结果。然后将第二次执行的结果 yielded （返回的是一个 Future 对象），包装成一个 YieldFuture 对象，然后通过 start()  函数处理：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def start(self, runner):</span><br><span class="line">    if not self.future.done():</span><br><span class="line">        self.runner = runner</span><br><span class="line">        self.key = object()</span><br><span class="line">        self.io_loop.add_future(self.future, runner.result_callback(self.key))</span><br><span class="line">    else:</span><br><span class="line">        self.runner = None</span><br><span class="line">        self.result = self.future.result()</span><br></pre></td></tr></table></figure>

<p>首先判断 future 是否被 set_done()，如果没有则注册一系列回调函数，如果完成则保存结果，以供下一次恢复执行时将结果送入生成器。<br>在 Runner.run() 执行完成后此时的 coroutine 中的 future 对象已经是被 set_done 的，然后直接返回 future 对象，最后被 外层的 @web.asynchronous 修饰器消费。</p>
<hr>
<p>参考：</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/MnCu8261/p/6560502.html">http://www.cnblogs.com/MnCu8261/p/6560502.html</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/chenchao1990/p/5406245.html">https://www.cnblogs.com/chenchao1990/p/5406245.html</a><br><a target="_blank" rel="noopener" href="http://blog.csdn.net/u010168160/article/details/53019039">http://blog.csdn.net/u010168160/article/details/53019039</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yezuhui/p/6863781.html">https://www.cnblogs.com/yezuhui/p/6863781.html</a><br><a target="_blank" rel="noopener" href="http://blog.csdn.net/zhaohongyan6/article/details/70888221">http://blog.csdn.net/zhaohongyan6/article/details/70888221</a><br><a target="_blank" rel="noopener" href="https://www.zybuluo.com/noogel/note/952488">https://www.zybuluo.com/noogel/note/952488</a></p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/10/08/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/10/08/1.html" class="post-title-link" itemprop="url">这些Mac神器也许你正需要</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-10-08 00:00:00" itemprop="dateCreated datePublished" datetime="2017-10-08T00:00:00+00:00">2017-10-08</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这篇文章主要介绍Mac下常用的效率工具，也许正是你所需要的或者使用后对你的工作有很大的效率提升，废话不多说，看下面介绍的五款常用效率工具。</p>
<h2 id="Alfred"><a href="#Alfred" class="headerlink" title="Alfred"></a>Alfred</h2><p> <br><img src="/resource/img/efficiency/a1.png" alt="Alfred"></p>
<p>Alfred 作为神器的霸主地位可谓实至名归，它不仅可以帮我们快速打开切换应用、打开网址，使用计算器、词典、剪贴板增强等功能，还可以通过Workflow模块实现功能的扩展，下面详细介绍一下此神器的一些功能。</p>
<p>首先我们定义调出 Alfred 的快捷键，这里我设置的是 Command + Space ，可以启动输入框。</p>
<p><img src="/resource/img/efficiency/a2.png" alt="Alfred"></p>
<p>在输入框中我们可以输入想要打开或切换的应用：</p>
<p><img src="/resource/img/efficiency/a3.png" alt="Alfred"></p>
<p>也可以输入基本的数学公式，计算结果：</p>
<p><img src="/resource/img/efficiency/a4.png" alt="Alfred"></p>
<p>或者去 Google 搜索：</p>
<p><img src="/resource/img/efficiency/a5.png" alt="Alfred"></p>
<p>打开 Terminal 执行命令：</p>
<p><img src="/resource/img/efficiency/a6.png" alt="Alfred"></p>
<p>在 Web Search 中配置自定义打开的网址：</p>
<p><img src="/resource/img/efficiency/a7.png" alt="Alfred"></p>
<p>调出剪贴板历史，我设置的快捷键是 Option + Command + C：</p>
<p><img src="/resource/img/efficiency/a8.png" alt="Alfred"></p>
<p>默认回车会执行第一个结果，或打开网址，或将结果复制到剪贴板，这样可以极大地提高我们操作的效率。<br> <br>当然这些默认的功能是不能够满足我们的，还可以通过 Worklow 去扩展效率工具，这里是自己做的一个效率工具箱 xyzUtils ：</p>
<p><img src="/resource/img/efficiency/a9.png" alt="Alfred"></p>
<p>Github 地址： <a target="_blank" rel="noopener" href="https://github.com/noogel/Alfred-Workflow">https://github.com/noogel/Alfred-Workflow</a></p>
<p>这里我做了一些开发中常用的数据转换功能，时间戳与时间的互相转换、Unicode码中文转换、随机字符串生成、IP查询、base64编码解码、MD5生成等，回车复制结果到剪贴板，举例如下：</p>
<p><img src="/resource/img/efficiency/a10.png" alt="Alfred"></p>
<p>网友们还提供了更多的 自定义功能，可自行知乎。</p>
<h2 id="Jietu"><a href="#Jietu" class="headerlink" title="Jietu"></a>Jietu</h2><p><img src="/resource/img/efficiency/j1.png" alt="Jietu"></p>
<p>这个是腾讯提供的免费截图工具，可进行区域截图或者屏幕录制功能，可以快捷编辑截图，也是我常用的一个神器。配置信息如下图：</p>
<p><img src="/resource/img/efficiency/j2.png" alt="Jietu"></p>
<p>截好图后可以按 空格键 进行快速编辑，很是方便，截图后会自动放到剪贴板，可直接粘贴到微信、QQ、Slack等应用的对话框中。</p>
<p><img src="/resource/img/efficiency/j3.png" alt="Jietu"></p>
<p> </p>
<h2 id="Hammerspoon"><a href="#Hammerspoon" class="headerlink" title="Hammerspoon"></a>Hammerspoon</h2><p><img src="/resource/img/efficiency/h1.png" alt="Hammerspoon"></p>
<p>这款神器和上面的 Alfred 功能点有些重合，可以提供快速启动应用、调整窗口大小等功能。通过自定义 Lua 脚本实现所需的功能，这些功能主要通过绑定快捷键实现功能出发，当然也会绑定一些系统事件触发脚本功能。</p>
<p>目前在网上搜集了一些基本功能，调整窗口比例，连接到办公区网络自动静音等功能。</p>
<p><img src="/resource/img/efficiency/h2.png" alt="Hammerspoon"></p>
<p> </p>
<h2 id="Time-Out"><a href="#Time-Out" class="headerlink" title="Time Out"></a>Time Out</h2><p><img src="/resource/img/efficiency/t1.png" alt="Time Out"></p>
<p>这款工具主要是可以帮助久用电脑的人每隔一段时间暂停一下，我这里设置的每隔 50分钟暂停 3分钟，就是在这 3分钟时间这个软件会弹出屏保提醒你稍事休息一下再工作，当然暂停期间是可以随之取消的，暂停的时候就是这个样子的。</p>
<p><img src="/resource/img/efficiency/t2.png" alt="Time Out"></p>
<h2 id="Reeder"><a href="#Reeder" class="headerlink" title="Reeder"></a>Reeder</h2><p>这是一款 Mac 上知名度很高的 RSS 阅读器，简洁的外观与便捷的操作方式可以省去了去个站点看文章。结合Mac 触控板的左右滑动操作还是很方便的。</p>
<p><img src="/resource/img/efficiency/r1.png" alt="Reeder"></p>
<p><img src="/resource/img/efficiency/r2.png" alt="Reeder"></p>
<p>最后，来还有一些常用效率工具会在在后面的文章继续介绍，或许正是你所需要的，敬请期待！</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/09/28/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/09/28/1.html" class="post-title-link" itemprop="url">深度学习系列第三篇 — MNIST数字识别</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2017-09-28T00:00:00+00:00">2017-09-28</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这一节将上一节学到的深度神经网络的概念运用起来，通过 tf 来实现 MNIST 手写字识别。<br>首先导入 tf 库和训练数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.examples.tutorials.mnist import input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(&quot;MNIST_data&quot;, one_hot=True)</span><br></pre></td></tr></table></figure>

<p>定义全局初始常量，其中 INPUT_NODE 数为每一张图片 28 * 28 的像素数，OUTPUT_NODE 就是分类的个数 10； LAYER1_NODE 为隐藏层节点数，BATCH_SIZE 为每次训练数据的个数；LEARNING_RATE_BASE 为基础学习率，LEARNING_RATE_DECAY 为学习率的衰减率，REGULARIZATION_RATE 为正则化损失函数的系数，TRAINING_STEPS 为训练的次数，MOVING_AVERAGE_DECAY 为滑动平均衰减率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">INPUT_NODE = 784</span><br><span class="line">OUTPUT_NODE = 10</span><br><span class="line"></span><br><span class="line">LAYER1_NODE = 500</span><br><span class="line">BATCH_SIZE = 100</span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = 0.8</span><br><span class="line">LEARNING_RATE_DECAY = 0.99</span><br><span class="line">REGULARIZATION_RATE = 0.0001</span><br><span class="line">TRAINING_STEPS = 30000</span><br><span class="line">MOVING_AVERAGE_DECAY = 0.99</span><br></pre></td></tr></table></figure>

<p>定义一个 inference 函数用来计算神经网络的向前传播结果，并且通过 RELU 函数实现了去线性化。avg_class 参数是用来支持测试时使用滑动平均模型，当我们使用了滑动平均模型时，weights 和 biases 值都是从 avg_class 中取出的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):</span><br><span class="line">    if avg_class is None:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)</span><br><span class="line">        return tf.matmul(layer1, weights2) + biases2</span><br><span class="line">    else:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))</span><br><span class="line">        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)</span><br></pre></td></tr></table></figure>

<p>定义输入层，生成隐藏层和输出层参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, [None, INPUT_NODE])</span><br><span class="line">y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE])</span><br><span class="line"></span><br><span class="line">weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))</span><br><span class="line">biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))</span><br><span class="line"></span><br><span class="line">weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))</span><br><span class="line">biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))</span><br></pre></td></tr></table></figure>

<p>计算当前参数下神经网络向前传播的效果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = inference(x, None, weights1, biases1, weights2, biases2)</span><br></pre></td></tr></table></figure>

<p>这里通过滑动平均衰减率和训练次数初始化这个类，用来加快训练早期变量的更新速度；global_step 为动态存储训练次数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">global_step = tf.Variable(0, trainable=False)</span><br><span class="line">variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br></pre></td></tr></table></figure>

<p>variables_averages_op 这里将所有的神经网络的上参数使用滑动平均，对于指定 trainable&#x3D;False 的参数不作用。计算使用了滑动平均模型处理的向前传播结果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">variables_averages_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)</span><br></pre></td></tr></table></figure>

<p>计算损失。交叉熵用来刻画预测值与真实值差距的损失函数，我们再通过 softmax 回归将结果变成概率分布。tf 提供了将这两个函数合并使用的函数，第一个参数是向前传播的结果，第二个参数是训练数据的答案。然后计算所有样例的交叉熵平均值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))</span><br><span class="line">cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br></pre></td></tr></table></figure>

<p>这里使用 L2 正则化损失函数，计算模型的正则化损失，计算权重的，不计算偏置。正则化损失函数用来避免过拟合。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line">regularization = regularizer(weights1) + regularizer(weights2)</span><br></pre></td></tr></table></figure>

<p>最后得出的总损失等于交叉熵损失和正则化损失之和。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = cross_entropy_mean + regularization</span><br></pre></td></tr></table></figure>

<p>设置指数衰减的学习率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learnging_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY)</span><br></pre></td></tr></table></figure>

<p>使用优化算法优化总损失。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(learnging_rate).minimize(loss, global_step=global_step)</span><br></pre></td></tr></table></figure>

<p>每过一次数据需要更新一下参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">with tf.control_dependencies([train_step, variables_averages_op]):</span><br><span class="line">    train_op = tf.no_op()</span><br></pre></td></tr></table></figure>

<p>检验使用了滑动平均模型的向前传播结果是否正确。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))</span><br></pre></td></tr></table></figure>

<p>计算平均准确率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br></pre></td></tr></table></figure>

<p>最后开始我们的训练，并验证数据的准确率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">with tf.Session() as sess:</span><br><span class="line">    # 初始化全部变量</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    # 准备验证数据</span><br><span class="line">    validate_feed = &#123;x: mnist.validation.images,</span><br><span class="line">                    y_: mnist.validation.labels&#125;</span><br><span class="line">    # 准备测试数据</span><br><span class="line">    test_feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125;</span><br><span class="line"></span><br><span class="line">    # 迭代</span><br><span class="line">    for i in range(TRAINING_STEPS):</span><br><span class="line">        if i % 1000 == 0:</span><br><span class="line">            # 使用全部的验证数据去做了验证</span><br><span class="line">            validate_acc = sess.run(accuracy, feed_dict=validate_feed)</span><br><span class="line">            print &quot;训练轮数：&quot;, i, &quot;，准确率：&quot;, validate_acc * 100, &quot;%&quot;</span><br><span class="line">        # 取出一部分训练数据</span><br><span class="line">        xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">        # 训练</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line"></span><br><span class="line">    # 计算最终的准确率。</span><br><span class="line">    test_acc = sess.run(accuracy, feed_dict=test_feed)</span><br><span class="line">    print &quot;训练轮数：&quot;, TRAINING_STEPS, &quot;，准确率：&quot;, test_acc * 100, &quot;%&quot;</span><br></pre></td></tr></table></figure>

<p>开始训练的过程，首先初始化所有变量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.global_variables_initializer().run()</span><br></pre></td></tr></table></figure>

<p>MNIST 数据分为训练数据、验证数据和测试数据。我们先准备好验证数据和测试数据，因为数据量不大，可以直接将全部数据用于训练。然后开始我们的迭代训练，训练数据有很多，我们每次训练只取一部分数据进行训练，这样减小计算量，加速神经网络的训练，又不会对结果产生太大影响。</p>
<p>tf 的训练通过  sess.run 函数，第一个参数是最终要计算的，也就是公式的输出，第二个参数 feed 是 placeholder 的输入。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;)</span><br></pre></td></tr></table></figure>

<p>通过一次次的训练，总损失会越来越小，模型的预测越来越准确，到达一个临界点。</p>
<p>完整代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">&quot;MNIST_data&quot;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MNIST数据集相关常数，其中输入节点数为每一张图片 28 * 28 的像素数，输出的节点数就是分类的个数 10； LAYER1_NODE 为隐藏层节点数，</span></span><br><span class="line"><span class="comment"># BATCH_SIZE 为每次训练数据的个数；LEARNING_RATE_BASE 为基础学习率，LEARNING_RATE_DECAY 为学习率的衰减率，</span></span><br><span class="line"><span class="comment"># REGULARIZATION_RATE 为正则化损失函数的系数，TRAINING_STEPS 为训练的次数，MOVING_AVERAGE_DECAY 为滑动平均衰减率</span></span><br><span class="line"></span><br><span class="line">INPUT_NODE = <span class="number">784</span></span><br><span class="line">OUTPUT_NODE = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">LAYER1_NODE = <span class="number">500</span></span><br><span class="line">BATCH_SIZE = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">LEARNING_RATE_BASE = <span class="number">0.8</span></span><br><span class="line">LEARNING_RATE_DECAY = <span class="number">0.99</span></span><br><span class="line">REGULARIZATION_RATE = <span class="number">0.0001</span></span><br><span class="line">TRAINING_STEPS = <span class="number">30000</span></span><br><span class="line">MOVING_AVERAGE_DECAY = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个函数用来计算神经网络的向前传播结果，并且通过 RELU 函数实现了去线性化。avg_class 参数是用来支持测试时使用滑动平均模型。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">inference</span>(<span class="params">input_tensor, avg_class, weights1, biases1, weights2, biases2</span>):</span><br><span class="line">    <span class="keyword">if</span> avg_class <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, weights2) + biases2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        layer1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1))</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入层</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, INPUT_NODE])</span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, OUTPUT_NODE])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成隐藏层参数</span></span><br><span class="line">weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">biases1 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[LAYER1_NODE]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成输出层参数</span></span><br><span class="line">weights2 = tf.Variable(tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=<span class="number">0.1</span>))</span><br><span class="line">biases2 = tf.Variable(tf.constant(<span class="number">0.1</span>, shape=[OUTPUT_NODE]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算当前参数下神经网络向前传播的效果。</span></span><br><span class="line">y = inference(x, <span class="literal">None</span>, weights1, biases1, weights2, biases2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这个变量用来存储当前训练的次数。</span></span><br><span class="line">global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里通过滑动平均衰减率和训练次数初始化这个类，用来加快训练早期变量的更新速度。</span></span><br><span class="line">variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里将所有的神经网络的上参数使用滑动平均，对于指定 trainable=False 的参数不作用。</span></span><br><span class="line">variables_averages_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算使用了滑动平均模型处理的向前传播结果。</span></span><br><span class="line">average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算交叉熵，用来刻画预测值与真实值差距的损失函数，第一个参数是向前传播的结果，第二个是训练数据的答案。</span></span><br><span class="line">cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有样例的交叉熵平均值。</span></span><br><span class="line">cross_entropy_mean = tf.reduce_mean(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 L2 正则化损失函数</span></span><br><span class="line">regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</span><br><span class="line"><span class="comment"># 计算模型的正则化损失，计算权重的，不计算偏置。</span></span><br><span class="line">regularization = regularizer(weights1) + regularizer(weights2)</span><br><span class="line"><span class="comment"># 总损失等于交叉熵损失和正则化损失之和。</span></span><br><span class="line">loss = cross_entropy_mean + regularization</span><br><span class="line"><span class="comment"># 设置指数衰减的学习率。</span></span><br><span class="line">learnging_rate = tf.train.exponential_decay(</span><br><span class="line">    LEARNING_RATE_BASE, global_step, mnist.train.num_examples / BATCH_SIZE, LEARNING_RATE_DECAY)</span><br><span class="line"><span class="comment"># 使用优化算法优化总损失。</span></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learnging_rate).minimize(loss, global_step=global_step)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每过一次数据需要更新一下参数。</span></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([train_step, variables_averages_op]):</span><br><span class="line">    train_op = tf.no_op()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检验使用了滑动平均模型的向前传播结果是否正确。</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(average_y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># 计算平均准确率。</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 初始化全部变量</span></span><br><span class="line">        tf.global_variables_initializer().run()</span><br><span class="line">        <span class="comment"># 准备验证数据</span></span><br><span class="line">        validate_feed = &#123;x: mnist.validation.images,</span><br><span class="line">                        y_: mnist.validation.labels&#125;</span><br><span class="line">        <span class="comment"># 准备测试数据</span></span><br><span class="line">        test_feed = &#123;x: mnist.test.images, y_: mnist.test.labels&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 迭代</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(TRAINING_STEPS):</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># 使用全部的验证数据去做了验证</span></span><br><span class="line">                validate_acc = sess.run(accuracy, feed_dict=validate_feed)</span><br><span class="line">                <span class="built_in">print</span> <span class="string">&quot;训练轮数：&quot;</span>, i, <span class="string">&quot;，准确率：&quot;</span>, validate_acc * <span class="number">100</span>, <span class="string">&quot;%&quot;</span></span><br><span class="line">            <span class="comment"># 取出一部分训练数据</span></span><br><span class="line">            xs, ys = mnist.train.next_batch(BATCH_SIZE)</span><br><span class="line">            <span class="comment"># 训练</span></span><br><span class="line">            sess.run(train_op, feed_dict=&#123;x: xs, y_: ys&#125;)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算最终的准确率。</span></span><br><span class="line">        test_acc = sess.run(accuracy, feed_dict=test_feed)</span><br><span class="line">        <span class="built_in">print</span> <span class="string">&quot;训练轮数：&quot;</span>, TRAINING_STEPS, <span class="string">&quot;，准确率：&quot;</span>, test_acc * <span class="number">100</span>, <span class="string">&quot;%&quot;</span></span><br></pre></td></tr></table></figure>

<pre><code>Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
训练轮数： 0 ，准确率： 9.20000001788 %
训练轮数： 1000 ，准确率： 97.619998455 %
训练轮数： 2000 ，准确率： 98.0799973011 %
训练轮数： 3000 ，准确率： 98.2599973679 %
训练轮数： 4000 ，准确率： 98.1999993324 %
训练轮数： 5000 ，准确率： 98.1800019741 %
训练轮数： 6000 ，准确率： 98.2400000095 %
训练轮数： 7000 ，准确率： 98.2200026512 %
训练轮数： 8000 ，准确率： 98.1999993324 %
训练轮数： 9000 ，准确率： 98.2599973679 %
训练轮数： 10000 ，准确率： 98.2400000095 %
训练轮数： 11000 ，准确率： 98.2400000095 %
训练轮数： 12000 ，准确率： 98.1599986553 %
训练轮数： 13000 ，准确率： 98.2599973679 %
训练轮数： 14000 ，准确率： 98.299998045 %
训练轮数： 15000 ，准确率： 98.4200000763 %
训练轮数： 16000 ，准确率： 98.2800006866 %
训练轮数： 17000 ，准确率： 98.3799993992 %
训练轮数： 18000 ，准确率： 98.3600020409 %
训练轮数： 19000 ，准确率： 98.3200013638 %
训练轮数： 20000 ，准确率： 98.3399987221 %
训练轮数： 21000 ，准确率： 98.3799993992 %
训练轮数： 22000 ，准确率： 98.400002718 %
训练轮数： 23000 ，准确率： 98.400002718 %
训练轮数： 24000 ，准确率： 98.4200000763 %
训练轮数： 25000 ，准确率： 98.3200013638 %
训练轮数： 26000 ，准确率： 98.4200000763 %
训练轮数： 27000 ，准确率： 98.3799993992 %
训练轮数： 28000 ，准确率： 98.400002718 %
训练轮数： 29000 ，准确率： 98.3200013638 %
训练轮数： 30000 ，准确率： 98.3900010586 %
</code></pre>
<blockquote>
<p>下一节总结 准确率、交叉熵平均值、总损失、学习率、平均绝对梯度 的变化趋势。</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/09/28/2.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/09/28/2.html" class="post-title-link" itemprop="url">深度学习系列第四篇 — 神经反向传播</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2017-09-28T00:00:00+00:00">2017-09-28</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95">反向传播算法 - 维基百科</a></p>
<p>反向传播是一种与最优化方法结合使用的，用来训练人工神经网络的常用方法。通过计算所有权值的损失函数梯度，反馈给最优化方法，用来更新权值以最小化损失函数。</p>
<p>反向传播算法分成两个阶段：激励传播和权值更新</p>
<ul>
<li>激励传播：将输入值送入网络，活的激励响应，验证结果求差，从而获得响应误差。</li>
<li>权重更新：将输入的激励和响应误差相乘，获得权重的梯度，将梯度乘一个比例后取反加到权重上。</li>
</ul>
<p>训练的过程就是不断的重复这两个阶段，直到获得满意的预测准确率。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/09/25/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/09/25/1.html" class="post-title-link" itemprop="url">深度学习系列第二篇 — 深度神经网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-09-25 00:00:00" itemprop="dateCreated datePublished" datetime="2017-09-25T00:00:00+00:00">2017-09-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>上一节学习的向前传播算法是一种线性模型，全连接神经网络和单层神经网络模型都只能处理线性问题，这具有相当大的局限性。而深度学习要强调的是非线性。</p>
<h3 id="激活函数去线性化"><a href="#激活函数去线性化" class="headerlink" title="激活函数去线性化"></a>激活函数去线性化</h3><p>如下图，如果我们将每一个神经元的输出通过一个非线性函数，那么这个神经网络模型就不再是线性的了，而这个非线性函数就是激活函数，也实现了我们对神经元的去线性化。</p>
<p><img src="/resource/img/deep_learning/jihuohanshu.jpg" alt="激活函数去线性化"></p>
<p>下面列举了三个常用激活函数</p>
<ul>
<li>ReLU 函数</li>
<li>sigmoid 函数</li>
<li>tanh 函数</li>
</ul>
<p><img src="/resource/img/deep_learning/jihuohanshu2.jpg" alt="激活函数去线性化"></p>
<p>tf 中也提供了这几种不同的非线性激活函数。</p>
<p><code>tf.nn.relu(tf.matmul(x, w1) + biases1)</code></p>
<p>通过对 x 的加权增加偏置项，再在外层加上激活函数，实现神经元的非线性化。</p>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数用来衡量预测值与真实值之间的不一致程度，是一个非负实值函数，损失函数越小，证明模型预测的越准确。</p>
<p>交叉熵可以用来衡量两个概率分布之间的距离，是分类问题中使用比较光的一种损失函数。对于两个概率分布 p 和 q，表示交叉熵如下：</p>
<p>$$H(p,q)&#x3D;-\sum_{x}p(x)log q(x)$$</p>
<p>将神经网络向前传播得到的结果变成概率分布使用 Softmax 回归，它可以作为一个算法来优化分类结果。假设神经网络的输出值为 <code>y1,y2,...yn</code>，那么 Softmax 回归处理的输出为：</p>
<p>$$softmax(y)<em>i&#x3D;y_i’&#x3D;\frac{e^{yi}}{\sum</em>{j&#x3D;1}^ne^{yj}}$$</p>
<p>如下图通过 Softmax 层将神经网络的输出变成一个概率分布。</p>
<p><img src="/resource/img/deep_learning/softmax.jpg" alt="Softmax"></p>
<p>交叉熵一般会与 Softmax 回归一起使用，tf 对这两个功能提供了封装提供函数 <code>tf.nn.softmax_cross_entropy_with_logits</code>。</p>
<p>对于回归问题区别与分类问题，需要预测的是一个任意实数，最常使用的损失函数是均方误差 MSE,定义如下：</p>
<p>$$MSE(y,y’)&#x3D;\frac{\sum_{i&#x3D;1}^n(y_i-y_i’)^2}{n}$$</p>
<h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p>反向传播算法是训练神经网络的核心算法，它可以根据定义好的损失函数优化神经网络的参数值，是神经网络模型的损失函数达到一个较小的值。</p>
<p>梯度下降算法是最常用的神经网络优化方法，假设用 θ 表示神经网络的参数， J(θ) 表示给定参数下的取值，梯度下降算法会迭代式的更新 θ，让迭代朝着损失最小的方向更新。梯度通过求偏导的方式计算，梯度为 $$\frac{∂}{∂θ}J(θ)$$ 然后定义一个学习率 η。参数更新公式如下：$$θ_{n+1}&#x3D;θ_n-η\frac{∂}{∂θ_n}J(θ_n)$$</p>
<p>优化过程分为两步：</p>
<ol>
<li>通过向前传播算法得到预测值，将预测值与真实值之间对比差距。</li>
<li>通过反向传播算法计算损失函数对每一个参数的梯度，根据梯度和学习率是梯度下降算法更新每一个参数。</li>
</ol>
<p>为了降低计算量和加速训练过程，可以使用随机梯度下降算法，选取一部分数据进行训练。</p>
<p>学习率的设置可以通过指数衰减法，逐步减小学习率，可以在开始时快速得到一个较优解，然后减小学习率，使后模型的训练更加稳定。tf 提供了<code>tf.train.exponential_decay</code> 函数实现指数衰减学习率， <code>每一轮优化的学习率 = 初始学习率 * 衰减系数 ^ (学习步数 / 衰减速度)</code></p>
<h3 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h3><p>通过损失函数优化模型参数的时候，并不是让模型尽量的模拟训练数据的行为，而是通过训练数据对未知数据给出判断，当一个模型能完美契合训练数据的时候，损失函数为0，但是无法对未知数据做出可靠的判断，这就是过拟合。</p>
<p>避免过拟合的常用方法是正则化，就是在损失函数中加入刻画模型复杂度的指标，我们对模型的优化则变为 $$J(θ)+λR(w)$$ 其中 <code>R(w)</code> 刻画的是模型的复杂程度，λ 表示模型复杂损失在总损失中的比例。下面是常用的两种正则化函数：</p>
<p>L1正则化：会让参数变得稀疏，公式不可导</p>
<p>$$R(w) &#x3D; \Vert<del>w</del>\Vert_1 &#x3D; \sum_i|w_i|$$</p>
<p>L2正则化：不会让参数变得稀疏，公式可导</p>
<p>$$R(w) &#x3D; \Vert<del>w</del>\Vert_2^2 &#x3D; \sum_i|w_i^2|$$</p>
<p>在实际使用中会将 L1 正则化和 L2 正则化同时使用：</p>
<p>$$R(w) &#x3D; \sum_iα|w_i|+(1-α)w_i^2$$</p>
<h3 id="滑动平均模型"><a href="#滑动平均模型" class="headerlink" title="滑动平均模型"></a>滑动平均模型</h3><p>在采用随机梯度下降算法训练神经网络时，使用平均滑动模型可以在大部分情况下提高模型在测试数据上的表现。在 tf 中提供了 <code>tf.train.ExponentialMovingAverage</code> 来实现这个模型，通过设置一个衰减率来初始化，在其中维护一个影子变量，可以控制模型的更新速度。<br><code>影子变量值 = 衰减率 * 影子变量值 + (1 - 衰减率) * 待更新变量</code>，为了让模型前期更新比较快，还提供了 num_updates 参数，每次使用的衰减率为：</p>
<p>$$min(decay,\frac{1+numupdates}{10+numupdates})$$</p>
<hr>
<p>附 mathjax 语法教程：<a target="_blank" rel="noopener" href="http://blog.csdn.net/u010945683/article/details/46757757">http://blog.csdn.net/u010945683/article/details/46757757</a></p>
<p>本章结束～</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/09/21/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/09/21/1.html" class="post-title-link" itemprop="url">深度学习系列第一篇 — 入门</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-09-21 00:00:00" itemprop="dateCreated datePublished" datetime="2017-09-21T00:00:00+00:00">2017-09-21</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这篇文章是学习人工智能的笔记，首先抛出下面一个问题。</p>
<h4 id="人工智能、机器学习和深度学习之间的关系？"><a href="#人工智能、机器学习和深度学习之间的关系？" class="headerlink" title="人工智能、机器学习和深度学习之间的关系？"></a>人工智能、机器学习和深度学习之间的关系？</h4><p><img src="/resource/img/deep_learning/relation.jpg" alt="三者之间的联系"></p>
<p>从图中可以看出人工智能概念的范围更广，机器学习是其中的一个子集，而深度学习又是机器学习中的一个子集。</p>
<p>深度学习的应用领域在计算机视觉、语音识别、自然语言处理和人机博弈方面比基于数理统计的机器学习会有更高的准确度。这里主要是做深度学习方面的学习笔记。</p>
<h2 id="深度学习的发展历程"><a href="#深度学习的发展历程" class="headerlink" title="深度学习的发展历程"></a>深度学习的发展历程</h2><p>深度发学习是深度神经网络的代名词，其起源于上世纪，只是在这几年开始火起来。</p>
<p>早期的神经网络模型类似于放生机器学习，由人类大脑的神经元演化出的神经元模型，见下图（图摘自《TensorFlow 实战Google深度学习框架》）：<br><img src="/resource/img/deep_learning/neuron.jpg" alt="对比图"></p>
<p>后面出现了感知机模型、分布式知识表达和神经网络反向传播算法，再到后来的卷积神经网络、循环神经网络和 LSTM 模型。</p>
<p>在神经网络发展的同时，传统机器学习算法的研究也在不断发展，上世纪 90 年代末逐步超越了神经网络，在当时相比之下传统机器学习算法有更好的识别准确率，而神经网络由于数据量和计算能力的限制发展比较困难。到了最近几年，由于云计算、GPU、大数据等的出现，为神经网络的发展做好了铺垫，AI 开始了一个新的时代。从自身出发，高中设立的一个比较模糊的梦想是参与到人工智能的研发当中去，所以大学学了计算机，当梦想与时代的发展有了一个交点，梦想的实现变得更加触手可及。</p>
<p>下面一张图表是对比主流深度学习开源工具（图摘自《TensorFlow 实战Google深度学习框架》）<br><img src="/resource/img/deep_learning/deeplearning_1.jpg" alt="d1"><br><img src="/resource/img/deep_learning/deeplearning_2.jpg" alt="d2"></p>
<p>目前我听说的比较多的两个开源工具是 Caffe 和 TensorFlow，然后去看了下 Github 关注量，前者 20.1k 后者 69.3k，TensorFlow 应该是目前最火的一个深度学习框架了吧。这篇文章包括后面的文章都会去记录 TensorFlow 的学习过程，下面的内容是介绍 TensorFlow 的基础概念，介绍中我尽量避免加入代码，因为目前 TensorFlow 更新比较快，发现好多写法在新的版本中不再被支持。</p>
<h2 id="TensorFlow-学起来"><a href="#TensorFlow-学起来" class="headerlink" title="TensorFlow 学起来"></a>TensorFlow 学起来</h2><p>TensorFlow 有两个重要概念 Tensor （张量）和 Flow（流），Tensor 表名的是数据结构，Flow 提现的是计算模型。</p>
<p><strong>PS:以下将 TensorFlow 简称为 tf</strong></p>
<h3 id="tf-计算模型-—-计算图"><a href="#tf-计算模型-—-计算图" class="headerlink" title="tf 计算模型 — 计算图"></a>tf 计算模型 — 计算图</h3><p>tf 通过计算图来表述计算的编程系统，其中的每个节点表示一个计算，不同点之间的连接表达依赖关系。下面几行代码表达一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">a = tf.constant([1.0, 2.0], name=&#x27;a&#x27;)</span><br><span class="line">b = tf.constant([2.0, 3.0], name=&#x27;b&#x27;)</span><br><span class="line">result = a + b</span><br></pre></td></tr></table></figure>

<p>上面的代码在计算图中会有 a、b、a + b三个节点。tf 默认会有一个全局的计算图，也可以生成新的计算图，<strong>不同计算图之间不会共享张量和运算</strong>。</p>
<p>计算图中可以通过<strong>集合</strong>的方式管理不同的资源，这些资源可以是张量、变量或者队列资源等。</p>
<h3 id="tf-数据模型-—-张量"><a href="#tf-数据模型-—-张量" class="headerlink" title="tf 数据模型 — 张量"></a>tf 数据模型 — 张量</h3><p>张量是 tf 管理和表示数据的形式，可以简单理解为多维数组。零阶张量表示标量，就是一个数；一阶张量是一个一维数组；n 阶张量是一个 n 维数组。 <code>tf.add(a, b, name=&#39;add&#39;)</code> 这行代码在运行时并不会得到结果，而是一个结果的引用，是一个张量的结构，包含三个属性 name、shape、dtype，name 仅仅是一个节点的名称；shape 是张量的维度，这个是一维的，长度为 2，这个属性比较重要；dtype 是数据类型，每个张量有唯一的数据类型，不同张量之间的计算需要保证类型统一。</p>
<p>上面的例子就是对两个常量做加法，然后生成计算结果的引用。</p>
<h3 id="tf-运行模型-—-会话"><a href="#tf-运行模型-—-会话" class="headerlink" title="tf 运行模型 — 会话"></a>tf 运行模型 — 会话</h3><p>tf 的会话用来执行定义好的运算，会话用来管理运行过程中的所有资源，计算完成后会帮助回收资源，通过 <code>with tf.Session() as sess:</code> 这种方式会在 with 结束时自动关闭回收资源。</p>
<p>通过 <code>tf.ConfigProto</code> 可以配置类似并行线程数、GPU分配策略、运算超时时间等参数，将配置添加到 <code>tf.Session</code> 中创建会话。</p>
<h3 id="向前传播算法"><a href="#向前传播算法" class="headerlink" title="向前传播算法"></a>向前传播算法</h3><p>通过全连接网络结构介绍，一个神经元有多个输入和一个输出，输出是不同输入的加权和，不同的输入权重就是神经网络的参数，神经网络的优化就是优化参数取值的过程。全连接网络结构是指相邻两层之间任意两个节点之间都有连接。</p>
<p>向前传播算法需要的三部分信息：</p>
<ul>
<li>第一部分从实体中取出特征向量作为输入。</li>
<li>第二部分是神经网络的连接结构，不同神经元之间的输入输出的连接关系。</li>
<li>第三部分是每个神经元中的参数。</li>
</ul>
<p>在 tf 中通过变量（<code>tf.Variable</code>）来保存和更新神经网络中的参数。</p>
<p><img src="/resource/img/deep_learning/xiangqianchuanbo.png" alt="xiangqianchuanbo"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># w1 是第一层，通过随机数生成一个 （2,3） 的矩阵</span></span><br><span class="line">w1 = tf.Variable(tf.random_normal([<span class="number">2</span>, <span class="number">3</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># w2 是第二层，通过随机石生成一个 （3,1） 的矩阵</span></span><br><span class="line">w2 = tf.Variable(tf.random_normal([<span class="number">3</span>, <span class="number">1</span>], stddev=<span class="number">1</span>, seed=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># x 是输入层，为一个 （1,2） 的矩阵</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="number">1</span>, <span class="number">2</span>), name=<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过 tensorflow 提供的矩阵相乘算法计算</span></span><br><span class="line">a = tf.matmul(x, w1)</span><br><span class="line">y = tf.matmul(a, w2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 这里初始化所有变量</span></span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="comment"># 通过 输入值 x 与 第一层的参数进行矩阵相乘，再与 第二层的参数进行矩阵相乘，实现神经网络的向前传播算法。</span></span><br><span class="line">    <span class="built_in">print</span> sess.run(y, feed_dict=&#123;x: [[<span class="number">0.7</span>, <span class="number">0.9</span>]]&#125;)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>[[ 3.95757794]]
</code></pre>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/08/17/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/08/17/1.html" class="post-title-link" itemprop="url">AlfredWorkflow</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-08-17 00:00:00" itemprop="dateCreated datePublished" datetime="2017-08-17T00:00:00+00:00">2017-08-17</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>开发工程师常用工具箱</p>
<h2 id="全局预览"><a href="#全局预览" class="headerlink" title="全局预览"></a>全局预览</h2><p><img src="/resource/img/Jietu20170817-204623.jpg" alt="1"></p>
<h2 id="支持的命令"><a href="#支持的命令" class="headerlink" title="支持的命令"></a>支持的命令</h2><p><img src="/resource/img/Jietu20170817-204906.jpg" alt="2"></p>
<ul>
<li><code>ntime</code> 时间戳转换，支持标准时间格式与时间戳自动检测转换，回车复制结果到剪贴板</li>
<li><code>nb64d</code> Base64 解码</li>
<li><code>nb64e</code> Base64 编码</li>
<li><code>nmd5</code> MD5 生成</li>
<li><code>ncny</code> 数字转人民币大写</li>
<li><code>nu2c</code> Unicode 码转中文</li>
<li><code>nc2u</code> 中文转 Unicode 码</li>
<li><code>nip</code> IP 地址查询</li>
<li><code>nrdm</code> 随机字符串生成，输入长度</li>
<li><code>nhelp</code> 列出所有支持的命令</li>
</ul>
<h4 id="欢迎大家补充。"><a href="#欢迎大家补充。" class="headerlink" title="欢迎大家补充。"></a>欢迎大家补充。</h4><h2 id="个人博客"><a href="#个人博客" class="headerlink" title="个人博客"></a>个人博客</h2><p><a target="_blank" rel="noopener" href="http://noogel.xyz/">知一的指纹</a></p>
<h2 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h2><p><a target="_blank" rel="noopener" href="https://github.com/noogel/Alfred-Workflow">Noogel’s github Alfred-Workflow</a></p>
<blockquote>
<p>Update at 2017-08-17</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/07/25/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/07/25/1.html" class="post-title-link" itemprop="url">主流PRC框架对比</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-07-25 00:00:00" itemprop="dateCreated datePublished" datetime="2017-07-25T00:00:00+00:00">2017-07-25</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>PRC 是指远程过程调用。即不同服务之间通过网络来表达调用的语义和传达数据。</p>
<h4 id="PRC-解决的问题"><a href="#PRC-解决的问题" class="headerlink" title="PRC 解决的问题"></a>PRC 解决的问题</h4><ul>
<li>解决通讯问题，一般通过 TCP 、 HTTP 连接来实现传输。</li>
<li>解决寻址问题。</li>
<li>实现技术异构。</li>
</ul>
<h4 id="RPC-的调用过程"><a href="#RPC-的调用过程" class="headerlink" title="RPC 的调用过程"></a>RPC 的调用过程</h4><p><img src="/resource/img/rpc_call_process.png" alt="RPC调用过程"></p>
<h4 id="PRC-框架有哪些？"><a href="#PRC-框架有哪些？" class="headerlink" title="PRC 框架有哪些？"></a>PRC 框架有哪些？</h4><ul>
<li>Thrift (Fackbook开源)</li>
<li>gRPC (Google开源)</li>
<li>hsf&#x2F;dubbo (阿里开源)</li>
<li>finagle (Twitter开源)</li>
</ul>
<p>我们如何选择一款合适的 RPC 框架。从实现技术异构的角度要支持夸语言的调用，并且在高并发的请求下能有很好的性能表现。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://noge.top/2017/07/19/1.html">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nuniok">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="码力欧">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2017/07/19/1.html" class="post-title-link" itemprop="url">读书计划 2017.07.17 ～ 2016.09.24</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2017-07-19 00:00:00" itemprop="dateCreated datePublished" datetime="2017-07-19T00:00:00+00:00">2017-07-19</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="微服务设计"><a href="#微服务设计" class="headerlink" title="微服务设计"></a>微服务设计</h2><p>共 12 章， 209 页。</p>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>微服务的优点和缺点？</p>
<p>如何衡量服务的划分？</p>
<p>服务测试如何全面覆盖，保障系统的稳定迭代？</p>
<p>如何有效监控系统指标，及时发现定位问题？</p>
<p>微服务的横向和纵向扩展策略，如何在钱包中扩展？</p>
<h4 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h4><p>7.17 ～ 7.23</p>
<ul>
<li>微服务</li>
<li>演化式架构师</li>
<li>如何建模服务</li>
<li>集成</li>
</ul>
<p>7.24 ～ 7.30</p>
<ul>
<li>分解单块系统</li>
<li>部署</li>
</ul>
<p>7.31 ～ 8.6</p>
<ul>
<li>测试</li>
<li>监控</li>
<li>安全</li>
</ul>
<p>8.7 ～ 8.13</p>
<ul>
<li>康威定律和系统设计</li>
<li>规模化微服务</li>
<li>总结</li>
</ul>
<h2 id="MySQL技术内幕"><a href="#MySQL技术内幕" class="headerlink" title="MySQL技术内幕"></a>MySQL技术内幕</h2><h4 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h4><p>慢查询优化</p>
<p>索引优化</p>
<p>对B+树的了解</p>
<p>锁的产生与如何防止死锁</p>
<h4 id="共五周"><a href="#共五周" class="headerlink" title="共五周"></a>共五周</h4><ol>
<li>第 一、二、三（日志文件） 章</li>
<li>表</li>
<li>索引与优化</li>
<li>锁</li>
<li>事务</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2015 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nuniok</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdn.jsdelivr.net/npm/mermaid@8.13.4/dist/mermaid.min.js","integrity":"sha256-96rwDGMWIQYB0yKGp1sKi1yrjrLPj2oT39IpbCsIrsg="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>



  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"nuniok","repo":"nuniok.github.io","client_id":"Ov23liWEs17fH5aYPIzS","client_secret":"6bbfa4b5a042d38b10c942b5c15f4d8a3895e982","admin_user":"nuniok","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"ef27d280b3878b0e79f751328d11303e"}</script>
<script src="/js/third-party/comments/gitalk.js"></script>

</body>
</html>
